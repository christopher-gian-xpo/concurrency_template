{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    " \n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import socket\n",
    "import multiprocessing\n",
    " \n",
    "class WebsiteDownException(Exception):\n",
    "    pass\n",
    " \n",
    "def ping_website(address, timeout=20):\n",
    "    \"\"\"\n",
    "    Check if a website is down. A website is considered down \n",
    "    if either the status_code >= 400 or if the timeout expires\n",
    "     \n",
    "    Throw a WebsiteDownException if any of the website down conditions are met\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.head(address, timeout=timeout)\n",
    "        if response.status_code >= 400:\n",
    "            logging.warning(\"Website %s returned status_code=%s\" % (address, response.status_code))\n",
    "            raise WebsiteDownException()\n",
    "    except requests.exceptions.RequestException:\n",
    "        logging.warning(\"Timeout expired for website %s\" % address)\n",
    "        raise WebsiteDownException()\n",
    "    \n",
    "def notify_owner(address):\n",
    "    \"\"\" \n",
    "    Send the owner of the address a notification that their website is down \n",
    "     \n",
    "    For now, we're just going to sleep for 0.5 seconds but this is where \n",
    "    you would send an email, push notification or text-message\n",
    "    \"\"\"\n",
    "    logging.info(\"Notifying the owner of %s website\" % address)\n",
    "    time.sleep(0.5)\n",
    "     \n",
    "def check_website(address):\n",
    "    \"\"\"\n",
    "    Utility function: check if a website is down, if so, notify the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ping_website(address)\n",
    "    except WebsiteDownException:\n",
    "        notify_owner(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBSITE_LIST = [\n",
    "     'http://envato.com',\n",
    "    'http://amazon.co.uk',\n",
    "    'http://amazon.com',\n",
    "    'http://facebook.com',\n",
    "    'http://google.com',\n",
    "    'http://google.fr',\n",
    "    'http://google.es',\n",
    "    'http://google.co.uk',\n",
    "    'http://internet.org',\n",
    "    'http://gmail.com',\n",
    "    'http://stackoverflow.com',\n",
    "    'http://github.com',\n",
    "    'http://heroku.com',\n",
    "    'http://really-cool-available-domain.com',\n",
    "    'http://djangoproject.com',\n",
    "    'http://rubyonrails.org',\n",
    "    'http://basecamp.com',\n",
    "    'http://trello.com',\n",
    "    'http://yiiframework.com',\n",
    "    'http://shopify.com',\n",
    "    'http://another-really-interesting-domain.co',\n",
    "    'http://airbnb.com',\n",
    "    'http://instagram.com',\n",
    "    'http://snapchat.com',\n",
    "    'http://youtube.com',\n",
    "    'http://baidu.com',\n",
    "    'http://yahoo.com',\n",
    "    'http://live.com',\n",
    "    'http://linkedin.com',\n",
    "    'http://yandex.ru',\n",
    "    'http://netflix.com',\n",
    "    'http://wordpress.com',\n",
    "    'http://bing.com'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
      "WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
      "WARNING:root:Website http://live.com returned status_code=405\n",
      "WARNING:root:Website http://netflix.com returned status_code=405\n",
      "WARNING:root:Website http://bing.com returned status_code=405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for SerialSquirrel: 17.596450090408325secs\n"
     ]
    }
   ],
   "source": [
    "# serial_squirrel.py\n",
    " \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    " \n",
    "for address in WEBSITE_LIST:\n",
    "    check_website(address)\n",
    "         \n",
    "end_time = time.time()        \n",
    " \n",
    "print(\"Time for SerialSquirrel: %ssecs\" % (end_time - start_time))\n",
    " \n",
    "# WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
    "# WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
    "# WARNING:root:Website http://bing.com returned status_code=405\n",
    "# Time for SerialSquirrel: 15.881232261657715secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading Approach\n",
    "We're going to get a bit more creative with the implementation of the threaded approach. We're using a queue to put the addresses in and create worker threads to get them out of the queue and process them. We're going to wait for the queue to be empty, meaning that all the addresses have been processed by our worker threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
      "WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
      "WARNING:root:Website http://live.com returned status_code=405\n",
      "WARNING:root:Website http://netflix.com returned status_code=405\n",
      "WARNING:root:Website http://bing.com returned status_code=405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for ThreadedSquirrel: 0.8774538040161133secs\n"
     ]
    }
   ],
   "source": [
    "# threaded_squirrel.py\n",
    " \n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    " \n",
    "NUM_WORKERS = 16\n",
    "task_queue = Queue()\n",
    " \n",
    "def worker():\n",
    "    # Constantly check the queue for addresses\n",
    "    while True:\n",
    "        address = task_queue.get()\n",
    "        check_website(address)\n",
    "         \n",
    "        # Mark the processed task as done\n",
    "        task_queue.task_done()\n",
    "        \n",
    "start_time = time.time()\n",
    "         \n",
    "# Create the worker threads\n",
    "threads = [Thread(target=worker) for _ in range(NUM_WORKERS)]\n",
    " \n",
    "# Add the websites to the task queue\n",
    "[task_queue.put(item) for item in WEBSITE_LIST]\n",
    " \n",
    "# Start all the workers\n",
    "[thread.start() for thread in threads]\n",
    " \n",
    "# Wait for all the tasks in the queue to be processed\n",
    "task_queue.join()\n",
    " \n",
    "         \n",
    "end_time = time.time()        \n",
    " \n",
    "print(\"Time for ThreadedSquirrel: %ssecs\" % (end_time - start_time))\n",
    " \n",
    "# WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
    "# WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
    "# WARNING:root:Website http://bing.com returned status_code=405\n",
    "# Time for ThreadedSquirrel: 3.110753059387207secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concurrent.futures\n",
    "As stated previously, concurrent.futures is a high-level API for using threads. The approach we're taking here implies using a ThreadPoolExecutor. We're going to submit tasks to the pool and get back futures, which are results that will be available to us in the future. Of course, we can wait for all futures to become actual results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
      "WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
      "WARNING:root:Website http://live.com returned status_code=405\n",
      "WARNING:root:Website http://netflix.com returned status_code=405\n",
      "WARNING:root:Website http://bing.com returned status_code=405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for FutureSquirrel: 3.81540584564209secs\n"
     ]
    }
   ],
   "source": [
    "# future_squirrel.py\n",
    " \n",
    "import time\n",
    "import concurrent.futures\n",
    " \n",
    "NUM_WORKERS = 16\n",
    " \n",
    "start_time = time.time()\n",
    " \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(check_website, address) for address in WEBSITE_LIST}\n",
    "    concurrent.futures.wait(futures)\n",
    "    \n",
    "end_time = time.time()        \n",
    " \n",
    "print(\"Time for FutureSquirrel: %ssecs\" % (end_time - start_time))\n",
    " \n",
    "# WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
    "# WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
    "# WARNING:root:Website http://bing.com returned status_code=405\n",
    "# Time for FutureSquirrel: 1.812899112701416secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multiprocessing Approach\n",
    "The multiprocessing library provides an almost drop-in replacement API for the threading library. In this case, we're going to take an approach more similar to the concurrent.futures one. We're setting up a multiprocessing.Pool and submitting tasks to it by mapping a function to the list of addresses (think of the classic Python map function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Timeout expired for website http://another-really-interesting-domain.co\n",
      "WARNING:root:Timeout expired for website http://really-cool-available-domain.com\n",
      "WARNING:root:Website http://live.com returned status_code=405\n",
      "WARNING:root:Website http://netflix.com returned status_code=405\n",
      "WARNING:root:Website http://bing.com returned status_code=405\n",
      "WARNING:root:Timeout expired for website http://linkedin.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for MultiProcessingSquirrel: 22.074260234832764secs\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 8\n",
    " \n",
    "start_time = time.time()\n",
    " \n",
    "with multiprocessing.Pool(processes=NUM_WORKERS) as pool:\n",
    "    results = pool.map_async(check_website, WEBSITE_LIST)\n",
    "    results.wait()\n",
    "\n",
    "end_time = time.time()         \n",
    "print(\"Time for MultiProcessingSquirrel: %ssecs\" % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concurrency",
   "language": "python",
   "name": "concurrency"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
